@inproceedings{berdicevskis-etal-2023-superlim,
    title = "Superlim: A {S}wedish Language Understanding Evaluation Benchmark",
    author = {Berdicevskis, Aleksandrs  and
      Bouma, Gerlof  and
      Kurtz, Robin  and
      Morger, Felix  and
      {\"O}hman, Joey  and
      Adesam, Yvonne  and
      Borin, Lars  and
      Dann{\'e}lls, Dana  and
      Forsberg, Markus  and
      Isbister, Tim  and
      Lindahl, Anna  and
      Malmsten, Martin  and
      Rekathati, Faton  and
      Sahlgren, Magnus  and
      Volodina, Elena  and
      B{\"o}rjeson, Love  and
      Hengchen, Simon  and
      Tahmasebi, Nina},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.506",
    doi = "10.18653/v1/2023.emnlp-main.506",
    pages = "8137--8153",
    abstract = "We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.",
}
